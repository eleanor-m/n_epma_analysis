{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buddingtonite - St Andrews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit WD scans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to find custom python package\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, \".\")\n",
    "sys.path.insert(1, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import readfiles, wdscan, correct_quant, calczaf, helper_funs\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------#### INPUT #### -----------------------------\n",
    "# Where is the data stored?\n",
    "raw_data_path = Path('../data/raw/buddingtonite_StA/raw_wd_scans')\n",
    "# What's the sample name?\n",
    "sample = 'buddingtonite'\n",
    "# Option to add peak position markers to plot:\n",
    "# e.g. pk_pos_markers = False (no markers)\n",
    "#      pk_pos_markers = [145.839] (one marker)\n",
    "#      pk_pos_markers = [145.84, 145.73] (two markers)\n",
    "pk_pos_markers =  [146.6] #\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = sorted(f for f in raw_data_path.glob('*') if f.stem[0] != '.') #Find all folders except hidden ones\n",
    "\n",
    "data = [None]*len(scans)\n",
    "comments = [None]*len(scans)\n",
    "fig = [None]*len(scans)\n",
    "\n",
    "for i, subfolder in enumerate(scans):\n",
    "\n",
    "    comments[i], data[i] = readfiles.import_jeol_wdscans(subfolder,\n",
    "                                                         scan_filename='data001_mm.csv',\n",
    "                                                         cnd_filename='data001.cnd',\n",
    "                                                         comment_line_num=80,\n",
    "                                                         sep=',')\n",
    "\n",
    "    fig, ax = wdscan.plot_wdscan(comments[i], data[i], cps_per_nA=False, save_to=None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fit all datasets together\n",
    "\n",
    "# Select regions of the spectrum to use in the fit\n",
    "fit_regions = [[120, 135], [155, 180]]\n",
    "\n",
    "# Perform the fit\n",
    "result, trimmed_data = wdscan.fit_scans_together(\n",
    "                                    data, fit_regions,\n",
    "                                    Path('../data/interim/buddingtonite_StA/fits/'),\n",
    "                                    max_c='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdscan.plot_fits_together(data, trimmed_data, result, comments,\n",
    "                          path_out=Path('../data/interim/buddingtonite_ANU/fits/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct quantitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['buddingtonite'] # List of samples in this dataset\n",
    "sample_folders = [\n",
    "    Path('../data/raw/buddingtonite_StA/raw_quant/buddingtonite_146,6')] # List of folders corresponding to the samples\n",
    "category = 'buddingtonite' # Category of this dataset (e.g. \"glasses\")\n",
    "\n",
    "wd_scan = Path('../data/interim/buddingtonite_StA/fits/key_params.txt') # Path to wd scan fit parameters\n",
    "std_dbase_info_file = Path('../data/_dictionaries/standards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = readfiles.find_files_and_folders(\n",
    "                samples, sample_folders,\n",
    "                apf_file=None,\n",
    "                # apf_file=Path('../data/_dictionaries/apf_values.csv'), #<- Can put None in here\n",
    "                wd_scan=wd_scan)\n",
    "\n",
    "datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot = [None] * len(datalist.folder)\n",
    "\n",
    "for i in range(len(datalist.folder)):\n",
    "    peak, bg, standard, info = readfiles.read_and_organise_data(\n",
    "                                    datalist.loc[i,:].copy(),\n",
    "                                    bgi=False,\n",
    "                                    save=False)\n",
    "    myspot[i] = correct_quant.Spot()\n",
    "    myspot[i].add_data(info, bg, peak, standard)\n",
    "    myspot[i].add_wd_scan_params_from_file(wd_scan)\n",
    "    print('Read dataset:', i + 1, 'of', len(datalist), ':',\n",
    "          myspot[i].info.comment)\n",
    "    myspot[i].comprehensify_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_quant.process_datasets(\n",
    "    myspot, \n",
    "    datalist, \n",
    "    num_mc_sims=100, \n",
    "    path_out=Path(\"../data/processed/buddingtonite_StA/background_corrections/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tables = correct_quant.write_summary_excel_tables(myspot, \"../data/processed/buddingtonite_StA/kraw_summaries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the spot objects out to a pickle file:\n",
    "with open('../data/interim/buddingtonite_StA/buddingtonite.pickle', 'wb') as handle:\n",
    "    pickle.dump(myspot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Write a spots summary table as an excel file:\n",
    "info_spots = []\n",
    "for spot in myspot:\n",
    "    info_spots.append(spot.info)\n",
    "\n",
    "info = pd.DataFrame(info_spots)\n",
    "info.to_csv('spots_info_' + category + '.csv')\n",
    "\n",
    "print('-----Finished-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write calczaf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot[0].standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot[0].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the saved pickle file ---------------------------\n",
    "saved_pickle_file = \"../data/interim/buddingtonite_StA/buddingtonite.pickle\"\n",
    "samples = ['buddingtonite']\n",
    "category = 'buddingtonite'\n",
    "subfolder = Path('../data/processed/buddingtonite_StA/calczaf_files/')\n",
    "\n",
    "write_detection_limit_calczaf_files = True\n",
    "detlim_subfolder = Path('../data/processed/buddingtonite_StA/calczaf_files/detlim/')\n",
    "\n",
    "# note: in the subfolder there must be a file specifying valence.\n",
    "# this can be copied from the _dictionaries folder.\n",
    "valence_dict = readfiles.read_valence_file(subfolder, pattern='valence*')\n",
    "standard_database_dict = pd.read_csv(\n",
    "    '../data/_dictionaries/standards.csv',\n",
    "     index_col=0, \n",
    "     header=None, \n",
    "     squeeze=True).to_dict()\n",
    "\n",
    "standard_database_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data -------------------------------------------------------------\n",
    "with open(saved_pickle_file, 'rb') as handle:\n",
    "    myspot = pickle.load(handle)\n",
    "\n",
    "print('Loaded data: ', [spot.info.comment for i, spot in enumerate(myspot)])\n",
    "\n",
    "# Separate the myspot list by sample\n",
    "sampledata = [None]*len(samples)\n",
    "for i, sample in enumerate(samples):\n",
    "    sampledata[i] = [spot for i, spot in enumerate(myspot) if sample == spot.info['sample']]\n",
    "\n",
    "sampledata = dict(zip(samples,sampledata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple different methods of processing the data, add a description\n",
    "run_descriptor = ['']  \n",
    "# Leave as a list of an empty string if not using: e.g. run_descriptor = ['']\n",
    "\n",
    "for i in range(len(samples)):\n",
    "\n",
    "    # Here we pass in these arguments as a dictionary - this is useful in order\n",
    "    # to reuse the arguments for the detection limit function. But you can\n",
    "    # alternatively pass in each argument just by defining it in the function\n",
    "    # as normal (see glasses example).\n",
    "\n",
    "    args = {\n",
    "              'elementByDifference' : None # string element symbol\n",
    "            , 'elementByStoichToStoichOxygen' : None # string element symbol\n",
    "            , 'stoichOxygenRatio' : 0\n",
    "            # for buddingtonite there is H\n",
    "            # that can be defined stoichiometrically relative to N:\n",
    "            , 'elementByStoichToOtherElement' : 'h'\n",
    "            , 'OtherElement' : 'n'\n",
    "            , 'stoichElementRatio' : 4\n",
    "\n",
    "            , 'correct_bg' : True\n",
    "            , 'correct_apf' : False\n",
    "\n",
    "            # Elements to omit from matrix correction\n",
    "            # (e.g. if analysed but not actually present in sample)\n",
    "            , 'remove_elements' : ['Rb','Mo','Ca','Mg']\n",
    "\n",
    "            , 'definedElements' : None # list of element symbols to add\n",
    "            , 'definedElementWts' : None # list of known element wt% to add\n",
    "            }\n",
    "\n",
    "    for j in range(len(run_descriptor)):\n",
    "\n",
    "        calczaf_path_out = subfolder / '{}{}.dat'.format(\n",
    "                                            samples[i], run_descriptor[j])\n",
    "        open(calczaf_path_out, 'w').close()  # Erase contents of file\n",
    "\n",
    "        if write_detection_limit_calczaf_files:\n",
    "            \n",
    "            helper_funs.make_folder_if_it_does_not_exist(detlim_subfolder)\n",
    "\n",
    "            detlim_path_out = detlim_subfolder / '{}{}_detlim.dat'.format(\n",
    "                                            samples[i], run_descriptor[j])\n",
    "            open(detlim_path_out, 'w').close()  # Erase contents of file\n",
    "\n",
    "        for spot in sampledata[samples[i]]:\n",
    "\n",
    "            calczaf.write_calczaf_input(\n",
    "                spot, calczaf_path_out, valence_dict, standard_database_dict,\n",
    "                accV=10, calcMode=2, taAngle=40, Oxide_or_Element=1,\n",
    "                **args) # <- **args unpacks the args dictionary defined earlier\n",
    "                # so that all those arguments are passed into the function\n",
    "                # without the need to type them all out.\n",
    "\n",
    "            if not args['correct_bg']:\n",
    "                msg = ('\\n\\nCalculating detection limit does not make sense'\n",
    "                    ' except on background-corrected data. Raw data files' \n",
    "                    ' contain an estimate of detection limit without bg'\n",
    "                    ' correction. To resolve this error, either set'\n",
    "                    ' write_detection_limit_calczaf_files = False, or'\n",
    "                    ' set correct_bg to True.\\n')\n",
    "\n",
    "                raise Exception(msg)\n",
    "\n",
    "            detlim_spot = correct_quant.create_detection_limit_spot(spot)\n",
    "\n",
    "            calczaf.write_calczaf_input(\n",
    "                detlim_spot, detlim_path_out, valence_dict, \n",
    "                standard_database_dict,\n",
    "                accV=10, calcMode=2, taAngle=40, Oxide_or_Element=1,\n",
    "                **args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so I need to find the values for the standards used at StA. Orthoclase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, but accorinng to my lab book none of these analyses were any good because of problems with the Si and Al values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n_epma_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
