{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyalophane analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to find custom python package\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(1, \".\")\n",
    "sys.path.insert(1, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import readfiles, wdscan, correct_quant, calczaf, helper_funs\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pyrolite.geochem\n",
    "\n",
    "from nb_helper_funs import compile_n_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrolite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major/trace elements other than N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyal_majors_raw = pd.read_csv(\"../data/raw/hyalophane_StA/hyalophane_majors/20211110_hyalophane_all.txt\",\n",
    "                header=1, index_col=False)[:5]\n",
    "\n",
    "mass_pct_cols = [col for col in hyal_majors_raw.columns if \"(Mass%)\" in col]\n",
    "hyal_majors = hyal_majors_raw.loc[:, mass_pct_cols].rename(columns={col: col.replace(\"(Mass%)\", \"\") for col in mass_pct_cols})\n",
    "hyal_majors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyal_majors_raw[[col for col in hyal_majors_raw.columns if \"(D.L.)\" in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyal_majors_element = hyal_majors[[\"SiO2\", \"Al2O3\", \"SrO\", \"Fe2O3\", \"K2O\", \"BaO\", \"Na2O\"]].pyrochem.convert_chemistry(\n",
    "    to=[\"Si\", \"Al\", \"Sr\", \"Fe\", \"K\", \"Ba\", \"Na\"]\n",
    ")\n",
    "\n",
    "hyal_majors_element[\"O\"] = hyal_majors[\"Total\"] - (hyal_majors_element.sum(axis=1))\n",
    "\n",
    "hyal_majors_element[\"Total\"] = hyal_majors_element.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyal_majors_summary = pd.concat([hyal_majors_element.T.mean(axis=1), hyal_majors_element.T.std(axis=1)], axis=1)\n",
    "hyal_majors_summary.columns = [\"wt% mean\", \"stdev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyal_majors_summary.round(2).to_csv(\"../data/processed/hyalophane_StA/hyalophane_majors_summary.csv\")\n",
    "hyal_majors_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyalophane\n",
    "* Analysed at St Andrews\n",
    "* Using GaN standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WD scan - visualise & fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------#### INPUT #### -----------------------------\n",
    "# Where is the data stored?\n",
    "scan_path = Path('../data/raw/hyalophane_StA/wd_scans/albite_22122021_0006_QLW/Pos_0001/')\n",
    "# What's the sample name?\n",
    "sample = 'hyalophane'\n",
    "# Option to add peak position markers to plot:\n",
    "pk_pos_markers =  [146.6] #\n",
    "# ---------------------------------------------------\n",
    "# Read in the data\n",
    "comments, data = readfiles.import_jeol_wdscans(\n",
    "    subfolder=Path(\"../data/raw/hyalophane_StA/wd_scans/hyalophane_22122021_0010_QLW/Pos_0002\"),\n",
    "    comment_line_num=80\n",
    ")\n",
    "\n",
    "# Plot the data without fitting\n",
    "wdscan.plot_wdscan(comments, data, save_to=\"../data/interim/hyalophane_StA/wd_scan.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and plot with the fits ------------\n",
    "# Choose parts of the spectrum to use in the fit\n",
    "bg_roi = [[120,135], [155, 180]]\n",
    "\n",
    "trimmed_data = wdscan.trim_data_from_regions(data, bg_roi)\n",
    "out = wdscan.fit_bg(trimmed_data)\n",
    "wdscan.plot_bg_fit(data, trimmed_data, out, sample, pk_pos_markers, save_to=Path(\"../data/interim/hyalophane_StA/fits\"))\n",
    "par_dict = wdscan.write_fit_params(out, sample, save_to=Path(\"../data/interim/hyalophane_StA/fits/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['hyalophane_GaNcalib', 'hyalophane_BNcalib'] # List of samples in this dataset\n",
    "sample_folders = [Path('../data/raw/hyalophane_StA/hyalophane_using_GaN_standard/'),\n",
    "                  Path('../data/raw/hyalophane_StA/hyalophane_using_BN_standard/')] # List of folders corresponding to the samples\n",
    "category = 'hyalophane' # Category of this dataset (e.g. \"glasses\")\n",
    "\n",
    "wd_scan = Path('../data/interim/hyalophane_StA/fits/key_params_hyalophane.txt') # Path to wd scan fit parameters\n",
    "std_dbase_info_file = Path('data/_dictionaries/standards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = readfiles.find_files_and_folders(\n",
    "                samples, sample_folders,\n",
    "                apf_file=None,\n",
    "                # apf_file=Path('../data/_dictionaries/apf_values.csv'), #<- Can put None in here\n",
    "                wd_scan=wd_scan\n",
    "                )\n",
    "\n",
    "# add the apf values separately because these come from different files for the \n",
    "# different calibrations\n",
    "\n",
    "datalist[\"apf\"] = [0.819] * 5 + [0.96] * 5\n",
    "datalist[\"apf_sd\"] = [0.013] * 5 + [0.02] * 5\n",
    "\n",
    "datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot = [None] * len(datalist.folder)\n",
    "\n",
    "for i in range(len(datalist.folder)):\n",
    "    peak, bg, standard, info = readfiles.read_and_organise_data(\n",
    "                                    datalist.loc[i,:].copy(),\n",
    "                                    bgi=False,\n",
    "                                    save=False)\n",
    "    myspot[i] = correct_quant.Spot()\n",
    "    myspot[i].add_data(info, bg, peak, standard)\n",
    "    myspot[i].add_wd_scan_params_from_file(wd_scan)\n",
    "    print('Read dataset:', i + 1, 'of', len(datalist), ':',\n",
    "          myspot[i].info.comment)\n",
    "    myspot[i].comprehensify_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_quant.process_datasets(\n",
    "    myspot, \n",
    "    datalist, \n",
    "    num_mc_sims=100, \n",
    "    path_out=Path(\"../data/processed/hyalophane_StA/background_corrections/\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make figure showing background correction and montecarlo simulation for hyalophane\n",
    "\n",
    "Next cell has modified code from Spot.correct_bg and Spot.correct_bg_mc method to make a prettier figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm  # For monte carlo number generation\n",
    "from src.correct_quant import compute_bg_positions, fit_quant_bg, lin_bg\n",
    "import matplotlib.transforms as mtransforms\n",
    "plt.rcParams[\"font.family\"] = \"arial\"\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "def plot_background_correction(\n",
    "        out, \n",
    "        pk_pos, \n",
    "        pk_cps,\n",
    "        bg_pos, \n",
    "        bg_cps, \n",
    "        figure_axis=None,\n",
    "        ):\n",
    "\n",
    "    newx = np.arange(120.0, 180.0, 0.2)\n",
    "    corrected_bg = out.eval(x=pk_pos)\n",
    "    fitted = out.eval(x=newx)\n",
    "\n",
    "    if figure_axis == None:\n",
    "        plt.figure()\n",
    "        ax = plt.axes()\n",
    "    else:\n",
    "        ax = figure_axis\n",
    "\n",
    "    # Add the measured data\n",
    "    plt.plot(bg_pos, bg_cps, 'ok', markersize=2)\n",
    "    plt.plot(newx, fitted, '-b', linewidth=0.5, alpha=0.5)\n",
    "    plt.plot(pk_pos, pk_cps, 'xk', linewidth=0.5)\n",
    "    plt.plot(newx, lin_bg(newx, bg_cps[[0, 2]], bg_pos[[0, 2]]), '-r',\n",
    "             linewidth=0.5)\n",
    "\n",
    "    ylims = ax.get_ylim()\n",
    "    plt.ylim(0, max(np.append(bg_cps, pk_cps)) +\n",
    "             0.1 * max(np.append(bg_cps, pk_cps)))\n",
    "\n",
    "    ylims = ax.get_ylim()\n",
    "    xlims = ax.get_xlim()\n",
    "    yrange = ylims[1] - ylims[0]\n",
    "    xrange = xlims[1] - xlims[0]\n",
    "\n",
    "    plt.xlabel('L (mm)')\n",
    "    plt.ylabel('cps')\n",
    "\n",
    "    return xlims, ylims\n",
    "\n",
    "\n",
    "\n",
    "# Select spot of interest to use\n",
    "spot = myspot[5]\n",
    "\n",
    "fig = plt.figure(figsize=(17*0.39, 7*0.39))\n",
    "mosaic = \"\"\"AB\"\"\"\n",
    "ax_dict = fig.subplot_mosaic(mosaic, sharey=True)\n",
    "\n",
    "for label, ax in ax_dict.items():\n",
    "    trans = mtransforms.ScaledTranslation(-5/72, -5/72, fig.dpi_scale_trans)\n",
    "    ax.text(1.0, 1.0, label, transform=ax.transAxes + trans,\n",
    "            fontsize=12, ha='right', va='top', fontfamily='Arial',\n",
    "            bbox=dict(facecolor='w', edgecolor='grey', pad=3.0))\n",
    "\n",
    "\n",
    "\n",
    "# === Normal background correction =====================================\n",
    "plt.sca(ax_dict[\"A\"])\n",
    "\n",
    "idx = spot.idx_N[0]  # index of the element to be corrected\n",
    "idx_dupl = spot.idx_N[1]  # index of the additional bg measurements\n",
    "\n",
    "# Get background positions for N as a numpy array\n",
    "bg_pos = compute_bg_positions(spot)\n",
    "\n",
    "# Get background cps values as a numpy array\n",
    "lwr_bg_cps = np.array(spot.bg.lwr_cps[idx:idx_dupl + 1])\n",
    "upr_bg_cps = np.array(spot.bg.upr_cps[idx:idx_dupl + 1])\n",
    "bg_cps = np.concatenate((lwr_bg_cps, upr_bg_cps), axis=0)\n",
    "\n",
    "# Fit the background according to its shape\n",
    "modelout = fit_quant_bg(bg_pos, bg_cps, spot.wd_scan_params)\n",
    "\n",
    "# Find the corrected background cps\n",
    "corrected_bg = modelout.eval(x=spot.peak.pos[idx])\n",
    "pk_cps_corrected = spot.peak.raw_cps[idx] - corrected_bg\n",
    "\n",
    "plot_background_correction(\n",
    "        modelout, spot.peak.pos[idx], spot.peak.raw_cps[idx], bg_pos,\n",
    "        bg_cps, figure_axis=ax_dict[\"A\"])\n",
    "\n",
    "# === Monte Carlo background correction =====================================\n",
    "plt.sca(ax_dict[\"B\"])\n",
    "\n",
    "num_mc_sims = 100\n",
    "\n",
    "idx = spot.idx_N[0]\n",
    "idx_dupl = spot.idx_N[1]\n",
    "\n",
    "# Create an array with rows being a mc-simulation of the peak cps\n",
    "pk_cps_raw = norm.rvs(loc=spot.peak.raw_cps[idx],\n",
    "                        scale=spot.peak.stdev_raw_cps[idx],\n",
    "                        size=num_mc_sims, random_state=None)\n",
    "\n",
    "# Get background positions based on relative position from peak\n",
    "bg_pos = compute_bg_positions(spot)\n",
    "\n",
    "# Get background cps values and stdevs\n",
    "lwr_bg_cps = np.array(spot.bg.lwr_cps[idx:idx_dupl + 1])\n",
    "upr_bg_cps = np.array(spot.bg.upr_cps[idx:idx_dupl + 1])\n",
    "bg_cps_means = np.concatenate((lwr_bg_cps, upr_bg_cps), axis=0)\n",
    "\n",
    "lwr_bg_stdev = np.array(spot.bg.stdev_lwr[idx:idx_dupl + 1])\n",
    "upr_bg_stdev = np.array(spot.bg.stdev_upr[idx:idx_dupl + 1])\n",
    "bg_cps_stdev = np.concatenate((lwr_bg_stdev, upr_bg_stdev), axis=0)\n",
    "\n",
    "# Create a matrix with rows being mc-simulations of the background\n",
    "bg_cps = np.empty(shape=(num_mc_sims, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    bg_cps[:, i] = norm.rvs(loc=bg_cps_means[i],\n",
    "                            scale=bg_cps_stdev[i],\n",
    "                            size=num_mc_sims, random_state=None)\n",
    "\n",
    "pk_cps_net_corrected = np.empty(shape=(num_mc_sims, 1))\n",
    "bg_cps_corrected = np.empty(shape=(num_mc_sims, 1))\n",
    "\n",
    "# Perform the background fits and plot a figure at the same time\n",
    "fit_params_mc = [None] * num_mc_sims\n",
    "\n",
    "for i in range(num_mc_sims):\n",
    "    # Fit the background according to its shape\n",
    "\n",
    "    # Print a loop counter\n",
    "    if i % 50 == 0: # If i is divisible by 50\n",
    "        print('monte-carlo loop {} of {}'.format(i, num_mc_sims))\n",
    "\n",
    "    modelout = fit_quant_bg(bg_pos, bg_cps[i, :], spot.wd_scan_params)\n",
    "\n",
    "    fit_params_mc[i] = [modelout.params['amplitude'].value,\n",
    "                        modelout.params['c'].value]\n",
    "\n",
    "    # Find the corrected background cps\n",
    "    bg_cps_corrected[i] = modelout.eval(x=spot.peak.pos[idx])\n",
    "    pk_cps_net_corrected[i] = pk_cps_raw[i] - bg_cps_corrected[i]\n",
    "\n",
    "    # Add this to the figure\n",
    "    xlims, ylims = plot_background_correction(\n",
    "        modelout, spot.peak.pos[idx], pk_cps_raw[i], bg_pos,\n",
    "        bg_cps[i, :], figure_axis=axs[1])\n",
    "\n",
    "mean_from_mc = [np.mean(pk_cps_net_corrected), np.mean(bg_cps_corrected)]\n",
    "stdev_pcnt_from_mc = [np.std(pk_cps_net_corrected) * 100 / mean_from_mc[0],\n",
    "                        np.std(bg_cps_corrected) * 100 / mean_from_mc[1]]\n",
    "\n",
    "for k in ax_dict.keys():\n",
    "\n",
    "    ax_dict[k].annotate(\n",
    "        text=r\"N K$\\alpha$\",\n",
    "        xy=(146.893, 60),\n",
    "        xytext=(146.893, 120),\n",
    "        ha=\"center\",\n",
    "        arrowprops={\"arrowstyle\": \"->\"}\n",
    "    )\n",
    "\n",
    "ax_dict[\"A\"].set_title(\"Background correction\", fontsize=10)\n",
    "ax_dict[\"B\"].set_title(\"Monte Carlo simulation\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../figures/fig_hyalophane_bg_correction_and_mc_simulation.png\", dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tables = correct_quant.write_summary_excel_tables(\n",
    "    myspot, \n",
    "    \"../data/processed/hyalophane_StA/kraw_summaries.xlsx\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the typical k-ratio values with different levels of correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typical_kratios = pd.DataFrame({\n",
    "    \"K-ratio\": [\n",
    "        summary_tables[0][\"original.kraw_pcnt\"].mean(),\n",
    "        summary_tables[0][\"montecarlo.kraw_pcnt\"].mean(),\n",
    "        summary_tables[0][\"montecarlo.kraw_apf_pcnt\"].mean()\n",
    "    ],\n",
    "    \"Stdev % (relative)\": [\n",
    "        max(\n",
    "            summary_tables[0][\"original.kraw_stdev_pcnt\"].mean(),\n",
    "            summary_tables[0][\"original.kraw_pcnt\"].std()\n",
    "            ),\n",
    "        max(\n",
    "            summary_tables[0][\"montecarlo.kraw_stdev_pcnt\"].mean(),\n",
    "            summary_tables[0][\"montecarlo.kraw_pcnt\"].std()\n",
    "            ),\n",
    "        max(\n",
    "            summary_tables[0][\"montecarlo.kraw_stdev_apf_pcnt\"].mean(),\n",
    "            summary_tables[0][\"montecarlo.kraw_apf_pcnt\"].std()\n",
    "            )\n",
    "    ]\n",
    "}, index = [\n",
    "    \"Original K-ratio (%)\", \n",
    "    \"Bg-corrected K-ratio (%)\", \n",
    "    \"Bg- and APF-corrected K-ratio (%)\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "typical_kratios.insert(\n",
    "    1, \n",
    "    column=\"Stdev (absolute)\", \n",
    "    value = typical_kratios[\"K-ratio\"] * typical_kratios[\"Stdev % (relative)\"] / 100\n",
    "    )\n",
    "\n",
    "typical_kratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write calczaf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['hyalophane_GaNcalib', 'hyalophane_BNcalib'] # List of samples in this dataset\n",
    "category = 'hyalophane'\n",
    "subfolder = Path('../data/processed/hyalophane_StA/calczaf_files/')\n",
    "\n",
    "write_detection_limit_calczaf_files = True\n",
    "detlim_subfolder = Path('../data/processed/hyalophane_StA/calczaf_files/detlim/')\n",
    "\n",
    "# note: in the subfolder there must be a file specifying valence.\n",
    "# this can be copied from the _dictionaries folder.\n",
    "valence_dict = readfiles.read_valence_file(subfolder, pattern='valence*')\n",
    "standard_database_dict = pd.read_csv(\n",
    "    '../data/_dictionaries/standards.csv',\n",
    "     index_col=0, \n",
    "     header=None, \n",
    "     squeeze=True).to_dict()\n",
    "\n",
    "standard_database_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary\n",
    "sampledata = [None]*len(samples)\n",
    "for i, sample in enumerate(samples):\n",
    "    sampledata[i] = [spot for i, spot in enumerate(myspot) if sample == spot.info['sample']]\n",
    "\n",
    "sampledata = dict(zip(samples,sampledata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyal_literature_vals = pd.Series(dict(\n",
    "    {\"Si\": 23.25,\n",
    "     \"Al\": 11.65,\n",
    "     \"K\":  5.68,\n",
    "     \"Na\": 1.11,\n",
    "     \"Ba\": 17.42,\n",
    "     \"Sr\": 0.29}\n",
    "))\n",
    "hyal_literature_vals\n",
    "\n",
    "\n",
    "# Or use actual analyses from StA\n",
    "\n",
    "hyal_element_vals = hyal_majors_summary.loc[[\"Si\", \"Al\", \"Sr\", \"Fe\", \"K\", \"Ba\", \"Na\"], \"wt% mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple different methods of processing the data, add a description\n",
    "run_descriptor = ['_1_base', '_2_bg', '_3_bg_apf']  \n",
    "# Leave as a list of an empty string if not using: e.g. run_descriptor = ['']\n",
    "\n",
    "for i in range(len(samples)):\n",
    "\n",
    "    # Here we pass in these arguments as a dictionary - this is useful in order\n",
    "    # to reuse the arguments for the detection limit function. But you can\n",
    "    # alternatively pass in each argument just by defining it in the function\n",
    "    # as normal (see glasses example).\n",
    "\n",
    "    args = {\n",
    "              'elementByDifference' : None # string element symbol\n",
    "            , 'elementByStoichToStoichOxygen' : None # string element symbol\n",
    "            , 'stoichOxygenRatio' : 0\n",
    "            # for hyalophane there is H\n",
    "            # that can be defined stoichiometrically relative to N:\n",
    "            , 'elementByStoichToOtherElement' : 'h'\n",
    "            , 'OtherElement' : 'n'\n",
    "            , 'stoichElementRatio' : 4\n",
    "\n",
    "            , 'correct_bg' : False\n",
    "            , 'correct_apf' : False\n",
    "\n",
    "            # Elements to omit from matrix correction\n",
    "            # (e.g. if analysed but not actually present in sample)\n",
    "            , 'remove_elements' : None\n",
    "\n",
    "            , 'definedElements' : hyal_element_vals.index # list of element symbols to add\n",
    "            , 'definedElementWts' : hyal_element_vals.values # list of known element wt% to add\n",
    "            }\n",
    "    \n",
    "    # Make copies of args with different values\n",
    "    args2 = args.copy()\n",
    "    args2[\"correct_bg\"] = True\n",
    "    args2[\"correct_apf\"] = False\n",
    "\n",
    "    args3 = args2.copy()\n",
    "    args3[\"correct_bg\"] = True\n",
    "    args3[\"correct_apf\"] = True\n",
    "\n",
    "    args_list = [args, args2, args3]\n",
    "\n",
    "    for j in range(len(run_descriptor)):\n",
    "        print(\"******************************************************\")\n",
    "        print(args_list[j][\"correct_bg\"], args_list[j][\"correct_apf\"])\n",
    "        print(\"******************************************************\")\n",
    "\n",
    "        calczaf_path_out = subfolder / '{}{}.dat'.format(\n",
    "                                            samples[i], run_descriptor[j])\n",
    "        open(calczaf_path_out, 'w').close()  # Erase contents of file\n",
    "\n",
    "        if write_detection_limit_calczaf_files:\n",
    "            \n",
    "            detlim_path_out = detlim_subfolder / '{}{}_detlim.dat'.format(\n",
    "                                            samples[i], run_descriptor[j])\n",
    "            open(detlim_path_out, 'w').close()  # Erase contents of file\n",
    "\n",
    "        for spot in sampledata[samples[i]]:\n",
    "\n",
    "            calczaf.write_calczaf_input(\n",
    "                spot, calczaf_path_out, valence_dict, standard_database_dict,\n",
    "                accV=10, calcMode=2, taAngle=40, Oxide_or_Element=1,\n",
    "                **args_list[j]) # <- **args unpacks the args dictionary defined earlier\n",
    "                # so that all those arguments are passed into the function\n",
    "                # without the need to type them all out.\n",
    "\n",
    "            if write_detection_limit_calczaf_files:\n",
    "                if args_list[j]['correct_bg']:\n",
    "\n",
    "                    detlim_spot = correct_quant.create_detection_limit_spot(spot)\n",
    "\n",
    "                    calczaf.write_calczaf_input(\n",
    "                        detlim_spot, detlim_path_out, valence_dict, \n",
    "                        standard_database_dict,\n",
    "                        accV=10, calcMode=2, taAngle=40, Oxide_or_Element=1,\n",
    "                        **args_list[j])\n",
    "                    \n",
    "                else:\n",
    "                    print('\\n\\nWarning: Not writing detection limit file.' \n",
    "                            'Calculating detection limit does not make sense'\n",
    "                            ' except on background-corrected data. Raw data files' \n",
    "                            ' contain an estimate of detection limit without bg'\n",
    "                            ' correction.\\n')\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual step - run through calczaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calczaf results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = Path('../data/processed/hyalophane_StA/calczaf_files/')\n",
    "\n",
    "helper_funs.check_calczaf_folder_exists(folderpath)\n",
    "valence_file = sorted(folderpath.glob('valence*'))[0]\n",
    "\n",
    "results = calczaf.process_calczaf_outputs(folderpath, valence_file)\n",
    "\n",
    "# For detection limits\n",
    "\n",
    "results_detlim = calczaf.process_calczaf_outputs(folderpath / 'detlim/', valence_file, detlim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_pct_summary_table = pd.concat(\n",
    "    {k: v[[\"average\", \"stdev\"]] for k, v in results[\"wtdata\"].items()},\n",
    "    axis=1\n",
    "    ).round(2)\n",
    "\n",
    "wt_pct_summary_table.to_csv(\"../data/processed/hyalophane_StA/wt_pct_summary.csv\")\n",
    "\n",
    "wt_pct_summary_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile a summary of nitrogen with stdevs on both individual and multiple measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_list = [\"1_base\", \"2_bg\", \"3_bg_apf\"]\n",
    "\n",
    "summary, details = compile_n_summary(\n",
    "    suffix_list, results, results_detlim, sampledata, datalist, summary_tables, samples\n",
    ")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(\"../data/processed/hyalophane_StA/N_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate effect of blank correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison with albite below, check out the raw cps and the magnitude of the bg correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_at_peak_pos = [s.montecarlo.loc[0, \"bg_cps_mc_mean\"] for s in myspot]\n",
    "print(\"Bg cps at peak position: {:.1f}\".format(np.mean(bg_at_peak_pos)))\n",
    "print(\n",
    "    \"Stdev of multiple measurements of bg cps: {:.1f}\".format(\n",
    "    np.mean(np.std(bg_at_peak_pos))\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Typical stdev on bg cps, \"\n",
    "    \"computed using montecarlo approach: \"\n",
    "    \"relative = {:.1f} %, absolute = {:.1f}\".format(\n",
    "    np.mean([s.montecarlo.loc[0, \"bg_cps_mc_stdev_pcnt\"] for s in myspot]),\n",
    "    np.mean([s.montecarlo.loc[0, \"bg_cps_mc_mean\"] * s.montecarlo.loc[0, \"bg_cps_mc_stdev_pcnt\"]/100 for s in myspot])\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cps = [s.peak.loc[0, \"raw_cps\"] for s in myspot]\n",
    "\n",
    "print(np.mean(raw_cps))\n",
    "(np.std(raw_cps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albite wavelength scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------#### INPUT #### -----------------------------\n",
    "# Where is the data stored?\n",
    "scan_path = Path('../data/raw/hyalophane_StA/wd_scans/albite_22122021_0006_QLW/Pos_0001/')\n",
    "# What's the sample name?\n",
    "sample = 'albite'\n",
    "# Option to add peak position markers to plot:\n",
    "# e.g. pk_pos_markers = False (no markers)\n",
    "#      pk_pos_markers = [145.839] (one marker)\n",
    "#      pk_pos_markers = [145.84, 145.73] (two markers)\n",
    "pk_pos_markers =  [145.839] #\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "comments, data = readfiles.import_jeol_wdscans(\n",
    "    subfolder=scan_path)\n",
    "\n",
    "# Plot the data without fitting\n",
    "wdscan.plot_wdscan(comments, data, save_to=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and plot with the fits ------------\n",
    "# Choose parts of the spectrum to use in the fit\n",
    "bg_roi = [[120,180]]\n",
    "\n",
    "trimmed_data = wdscan.trim_data_from_regions(data, bg_roi)\n",
    "out = wdscan.fit_bg(trimmed_data)\n",
    "wdscan.plot_bg_fit(data, trimmed_data, out, sample, pk_pos_markers, save_to=None)\n",
    "par_dict = wdscan.write_fit_params(out, sample, save_to=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlay hyalophane and albite wavelength scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_path = Path('../data/raw/hyalophane_StA/wd_scans/hyalophane_22122021_0010_QLW/Pos_0002/')\n",
    "_, hyal_data = readfiles.import_jeol_wdscans(\n",
    "    subfolder=scan_path)\n",
    "scan_path = Path('../data/raw/hyalophane_StA/wd_scans/albite_22122021_0006_QLW/Pos_0001/')\n",
    "_, albite_data = readfiles.import_jeol_wdscans(\n",
    "    subfolder=scan_path)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.scatter(albite_data.L, albite_data.cps_per_nA, marker='.', color='k', s=10, label=\"Albite\")\n",
    "plt.scatter(hyal_data.L, hyal_data.cps_per_nA, marker='.', color='r', s=10, label=\"Hyalophane\")\n",
    "lgnd = plt.legend(loc=\"upper right\", scatterpoints=1, fontsize=10)\n",
    "for handle in lgnd.legend_handles:\n",
    "    handle.set_sizes([100.0])\n",
    "\n",
    "ylims = ax.get_ylim()\n",
    "plt.ylim(-0.1, ylims[1])\n",
    "\n",
    "pk_pos = [146.4, 146.8]\n",
    "\n",
    "plt.fill_between([pk_pos[0], pk_pos[1], pk_pos[1], pk_pos[0]],\n",
    "                    [-0.1, -0.1, ylims[1], ylims[1]], alpha=0.2, color='grey',\n",
    "                    linewidth=0)\n",
    "\n",
    "plt.xlabel('L (mm)')\n",
    "\n",
    "plt.ylabel('cps/nA')\n",
    "plt.xlim(118, 182)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albite quant analyses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of an albite quant analysis was that if you measure on peak, you can use that value as a background for hyalophane. In principle..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, albite doesn't have 17 wt% Ba though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['albite'] # List of samples in this dataset\n",
    "sample_folders = [Path('../data/raw/hyalophane_StA/albite_quant/')] # List of folders corresponding to the samples\n",
    "category = 'albite' # Category of this dataset (e.g. \"glasses\")\n",
    "\n",
    "wd_scan = Path('data/interim/buddingtonite_ANU/fits/key_params_buddingtonite.txt') # Path to wd scan fit parameters\n",
    "std_dbase_info_file = Path('data/_dictionaries/standards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = readfiles.find_files_and_folders(\n",
    "                samples, sample_folders,\n",
    "                # apf_file=Path('data/_dictionaries/apf_values.csv'), #<- Can put None in here\n",
    "                # wd_scan=wd_scan\n",
    "                )\n",
    "\n",
    "datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot = [None] * len(datalist.folder)\n",
    "\n",
    "for i in range(len(datalist.folder)):\n",
    "    peak, bg, standard, info = readfiles.read_and_organise_data(\n",
    "                                    datalist.loc[i,:].copy(),\n",
    "                                    bgi=False,\n",
    "                                    save=False)\n",
    "    myspot[i] = correct_quant.Spot()\n",
    "    myspot[i].add_data(info, bg, peak, standard)\n",
    "#   myspot[i].add_wd_scan_params_from_file(wd_scan)\n",
    "    print('Read dataset:', i + 1, 'of', len(datalist), ':',\n",
    "          myspot[i].info.comment)\n",
    "    myspot[i].comprehensify_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot[0].peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot[0].standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so I only analysed nitrogen in this albite. \n",
    "\n",
    "I have five analyses for which all have raw cps values. \n",
    "\n",
    "So I can use these raw cps values (at the peak position) to correct the raw cps values for hyalophane as an alternative to the background correction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.peak.loc[0, \"raw_cps\"] for s in myspot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the raw cps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([s.peak.loc[0, \"raw_cps\"] for s in myspot]))\n",
    "(np.std([s.peak.loc[0, \"raw_cps\"] for s in myspot]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the typical raw cps in those albite analyses was 33.1(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, from above, the typical background correction using the montecarlo background fitting approach on hyalophane directly was 35.0(3) (stdev on multiple measurements; this was larger than stdev on individual measurements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So....\n",
    "this means that on average if we used the albite as a background correction for hyalophane, we would gain two cps on the net background counts, given that these were about 10, this is an increase of about 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we could expect the N concentrations to be about 20% larger if we did this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n_epma_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
