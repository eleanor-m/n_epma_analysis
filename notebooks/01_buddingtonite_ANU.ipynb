{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buddingtonite - ANU\n",
    "\n",
    "This notebook contains the analysis for buddingtonite measured at the JEOL probe at ANU.\n",
    "\n",
    "## Fit WD scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to find custom python package\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(1, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import readfiles, wdscan, correct_quant, calczaf, helper_funs\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from nb_helper_funs import compile_n_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------#### INPUT #### -----------------------------\n",
    "# Where is the data stored?\n",
    "scan_path = Path('../data/raw/buddingtonite_ANU/raw_wd_scans')\n",
    "# What's the sample name?\n",
    "sample = 'buddingtonite'\n",
    "# Option to add peak position markers to plot:\n",
    "# e.g. pk_pos_markers = False (no markers)\n",
    "#      pk_pos_markers = [145.839] (one marker)\n",
    "#      pk_pos_markers = [145.84, 145.73] (two markers)\n",
    "pk_pos_markers =  [145.839] #\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "comments, data = readfiles.import_jeol_wdscans(scan_path)\n",
    "\n",
    "# Plot the data without fitting\n",
    "wdscan.plot_wdscan(comments, data, save_to=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and plot with the fits ------------\n",
    "# Choose parts of the spectrum to use in the fit\n",
    "bg_roi = [[120,125], [130,138], [158,180]]\n",
    "\n",
    "trimmed_data = wdscan.trim_data_from_regions(data, bg_roi)\n",
    "out = wdscan.fit_bg(trimmed_data)\n",
    "wdscan.plot_bg_fit(data, trimmed_data, out, sample, pk_pos_markers, save_to=None)\n",
    "par_dict = wdscan.write_fit_params(out, sample, save_to=Path(\"../data/interim/buddingtonite_ANU/fits/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct quantitative analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['buddingtonite'] # List of samples in this dataset\n",
    "sample_folders = [Path('../data/raw/buddingtonite_ANU/raw_quant/')] # List of folders corresponding to the samples\n",
    "category = 'buddingtonite' # Category of this dataset (e.g. \"glasses\")\n",
    "\n",
    "wd_scan = Path('../data/interim/buddingtonite_ANU/fits/key_params_buddingtonite.txt') # Path to wd scan fit parameters\n",
    "std_dbase_info_file = Path('../data/_dictionaries/standards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = readfiles.find_files_and_folders(\n",
    "                samples, sample_folders,\n",
    "                apf_file=Path('../data/_dictionaries/apf_values.csv'), #<- Can put None in here\n",
    "                wd_scan=wd_scan)\n",
    "\n",
    "datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot = [None] * len(datalist.folder)\n",
    "\n",
    "for i in range(len(datalist.folder)):\n",
    "    peak, bg, standard, info = readfiles.read_and_organise_data(\n",
    "                                    datalist.loc[i,:].copy(),\n",
    "                                    bgi=False,\n",
    "                                    save=False)\n",
    "    myspot[i] = correct_quant.Spot()\n",
    "    myspot[i].add_data(info, bg, peak, standard)\n",
    "    myspot[i].add_wd_scan_params_from_file(wd_scan)\n",
    "    print('Read dataset:', i + 1, 'of', len(datalist), ':',\n",
    "          myspot[i].info.comment)\n",
    "    myspot[i].comprehensify_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_quant.process_datasets(myspot, datalist, num_mc_sims=100, path_out=Path(\"../data/processed/buddingtonite_ANU/background_corrections/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tables = correct_quant.write_summary_excel_tables(myspot, \"../data/processed/buddingtonite_ANU/kraw_summaries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the spot objects out to a pickle file:\n",
    "with open('../data/interim/buddingtonite_ANU/buddingtonite.pickle', 'wb') as handle:\n",
    "    pickle.dump(myspot, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Write a spots summary table as an excel file:\n",
    "info_spots = []\n",
    "for spot in myspot:\n",
    "    info_spots.append(spot.info)\n",
    "\n",
    "info = pd.DataFrame(info_spots)\n",
    "info.to_csv('spots_info_' + category + '.csv')\n",
    "\n",
    "print('-----Finished-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write calczaf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the saved pickle file ---------------------------\n",
    "saved_pickle_file = \"../data/interim/buddingtonite_ANU/buddingtonite.pickle\"\n",
    "samples = ['buddingtonite']\n",
    "category = 'buddingtonite'\n",
    "subfolder = Path('../data/processed/buddingtonite_ANU/calczaf_files/')\n",
    "\n",
    "write_detection_limit_calczaf_files = True\n",
    "detlim_subfolder = Path('../data/processed/buddingtonite_ANU/calczaf_files/detlim/')\n",
    "\n",
    "# note: in the subfolder there must be a file specifying valence.\n",
    "# this can be copied from the _dictionaries folder.\n",
    "valence_dict = readfiles.read_valence_file(subfolder, pattern='valence*')\n",
    "standard_database_dict = pd.read_csv(\n",
    "    '../data/_dictionaries/standards.csv',\n",
    "     index_col=0, \n",
    "     header=None, \n",
    "     squeeze=True).to_dict()\n",
    "\n",
    "standard_database_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data -------------------------------------------------------------\n",
    "with open(saved_pickle_file, 'rb') as handle:\n",
    "    myspot = pickle.load(handle)\n",
    "\n",
    "print('Loaded data: ', [spot.info.comment for i, spot in enumerate(myspot)])\n",
    "\n",
    "# Separate the myspot list by sample\n",
    "sampledata = [None]*len(samples)\n",
    "for i, sample in enumerate(samples):\n",
    "    sampledata[i] = [spot for i, spot in enumerate(myspot) if sample == spot.info['sample']]\n",
    "\n",
    "sampledata = dict(zip(samples,sampledata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple different methods of processing the data, add a description\n",
    "run_descriptor = ['_1_base', '_2_bg', '_3_bg_apf']  \n",
    "# Leave as a list of an empty string if not using: e.g. run_descriptor = ['']\n",
    "\n",
    "for i in range(len(samples)):\n",
    "\n",
    "    # Here we pass in these arguments as a dictionary - this is useful in order\n",
    "    # to reuse the arguments for the detection limit function. But you can\n",
    "    # alternatively pass in each argument just by defining it in the function\n",
    "    # as normal (see glasses example).\n",
    "\n",
    "    args = {\n",
    "              'elementByDifference' : None # string element symbol\n",
    "            , 'elementByStoichToStoichOxygen' : None # string element symbol\n",
    "            , 'stoichOxygenRatio' : 0\n",
    "            # for buddingtonite there is H\n",
    "            # that can be defined stoichiometrically relative to N:\n",
    "            , 'elementByStoichToOtherElement' : 'h'\n",
    "            , 'OtherElement' : 'n'\n",
    "            , 'stoichElementRatio' : 4\n",
    "\n",
    "            , 'correct_bg' : False\n",
    "            , 'correct_apf' : False\n",
    "\n",
    "            # Elements to omit from matrix correction\n",
    "            # (e.g. if analysed but not actually present in sample)\n",
    "            , 'remove_elements' : ['Rb','Mo','Ca','Mg']\n",
    "\n",
    "            , 'definedElements' : None # list of element symbols to add\n",
    "            , 'definedElementWts' : None # list of known element wt% to add\n",
    "            }\n",
    "    \n",
    "    # Make copies of args with different values\n",
    "    args2 = args.copy()\n",
    "    args2[\"correct_bg\"] = True\n",
    "    args2[\"correct_apf\"] = False\n",
    "\n",
    "    args3 = args2.copy()\n",
    "    args3[\"correct_bg\"] = True\n",
    "    args3[\"correct_apf\"] = True\n",
    "\n",
    "    args_list = [args, args2, args3]\n",
    "\n",
    "    for j in range(len(run_descriptor)):\n",
    "        print(\"******************************************************\")\n",
    "        print(args_list[j][\"correct_bg\"], args_list[j][\"correct_apf\"])\n",
    "        print(\"******************************************************\")\n",
    "\n",
    "        calczaf_path_out = subfolder / '{}{}.dat'.format(\n",
    "                                            samples[i], run_descriptor[j])\n",
    "        open(calczaf_path_out, 'w').close()  # Erase contents of file\n",
    "\n",
    "        if write_detection_limit_calczaf_files:\n",
    "            \n",
    "            detlim_path_out = detlim_subfolder / '{}{}_detlim.dat'.format(\n",
    "                                            samples[i], run_descriptor[j])\n",
    "            open(detlim_path_out, 'w').close()  # Erase contents of file\n",
    "\n",
    "        for spot in sampledata[samples[i]]:\n",
    "\n",
    "            calczaf.write_calczaf_input(\n",
    "                spot, calczaf_path_out, valence_dict, standard_database_dict,\n",
    "                accV=10, calcMode=2, taAngle=40, Oxide_or_Element=1,\n",
    "                **args_list[j]) # <- **args unpacks the args dictionary defined earlier\n",
    "                # so that all those arguments are passed into the function\n",
    "                # without the need to type them all out.\n",
    "\n",
    "            if write_detection_limit_calczaf_files:\n",
    "                if args_list[j]['correct_bg']:\n",
    "\n",
    "                    detlim_spot = correct_quant.create_detection_limit_spot(spot)\n",
    "\n",
    "                    calczaf.write_calczaf_input(\n",
    "                        detlim_spot, detlim_path_out, valence_dict, \n",
    "                        standard_database_dict,\n",
    "                        accV=10, calcMode=2, taAngle=40, Oxide_or_Element=1,\n",
    "                        **args_list[j])\n",
    "                    \n",
    "                else:\n",
    "                    print('\\n\\nWarning: Not writing detection limit file.' \n",
    "                            'Calculating detection limit does not make sense'\n",
    "                            ' except on background-corrected data. Raw data files' \n",
    "                            ' contain an estimate of detection limit without bg'\n",
    "                            ' correction.\\n')\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual step - run the file through calczaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process calczaf outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import calczaf, helper_funs\n",
    "from pathlib import Path\n",
    "\n",
    "folderpath = Path('../data/processed/buddingtonite_ANU/calczaf_files/')\n",
    "\n",
    "helper_funs.check_calczaf_folder_exists(folderpath)\n",
    "valence_file = sorted(folderpath.glob('valence*'))[0]\n",
    "\n",
    "results = calczaf.process_calczaf_outputs(folderpath, valence_file)\n",
    "\n",
    "# For detection limits\n",
    "\n",
    "results_detlim = calczaf.process_calczaf_outputs(folderpath / 'detlim/', valence_file, detlim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot[0].montecarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(\n",
    "    original_raw_cps = np.mean([s.peak.loc[8, \"raw_cps\"] for s in myspot]),\n",
    "    linear_bg_net_cps = np.mean([s.peak.loc[8, \"net_cps\"] for s in myspot]),\n",
    "    curved_bg_net_cps = np.mean([s.corrected.loc[8, \"net_cps\"] for s in myspot]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(\n",
    "    linear_bg_kraw = np.mean([s.peak.loc[8, \"kraw_pcnt\"] for s in myspot]),\n",
    "    curved_bg_kraw = np.mean([s.corrected.loc[8, \"kraw_pcnt\"] for s in myspot]),\n",
    "    curved_bg_apf_kraw = np.mean([s.montecarlo.loc[8, \"kraw_apf_pcnt\"] for s in myspot])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_pct_summary_table = pd.concat(\n",
    "    {k: v[[\"average\", \"stdev\"]] for k, v in results[\"wtdata\"].items()},\n",
    "    axis=1\n",
    "    ).round(2)\n",
    "\n",
    "wt_pct_summary_table.to_csv(\"../data/processed/buddingtonite_ANU/wt_pct_summary.csv\")\n",
    "\n",
    "wt_pct_summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"wtdata\"][\"buddingtonite_3_bg_apf\"][[\"average\", \"stdev\"]].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate expected N wt% based on stoichiometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import periodictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budd = periodictable.formula(\"NH4AlSi3O8\") + 0.5*periodictable.formula(\"H2O\")\n",
    "budd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    budd.atoms,\n",
    "    dict(zip(\n",
    "        budd.atoms.keys(),     \n",
    "        [periodictable.nitrogen.mass,\n",
    "            periodictable.hydrogen.mass,\n",
    "            periodictable.aluminum.mass,\n",
    "            periodictable.silicon.mass,\n",
    "            periodictable.oxygen.mass\n",
    "        ]\n",
    "    ))\n",
    "]\n",
    ").T\n",
    "\n",
    "df.columns = [\"n_mols\", \"molar_mass\"]\n",
    "\n",
    "df\n",
    "df[\"wt\"] = df[\"n_mols\"] * df[\"molar_mass\"]\n",
    "df[\"wt%\"] = (df[\"wt\"] / df[\"wt\"].sum())*100\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stdev on individual measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledata[\"buddingtonite\"][0].peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_list = [\"1_base\", \"2_bg\", \"3_bg_apf\"]\n",
    "\n",
    "summary, details = compile_n_summary(\n",
    "    suffix_list, results, results_detlim, sampledata, datalist, summary_tables, samples\n",
    ")\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(\"../data/processed/buddingtonite_ANU/nitrogen_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n_epma_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
