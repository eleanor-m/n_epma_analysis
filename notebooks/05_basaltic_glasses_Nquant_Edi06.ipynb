{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edi06 - Nitrogen quantitative analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up to find custom python package\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.insert(1, \".\")\n",
    "sys.path.insert(1, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import readfiles, wdscan, correct_quant, calczaf, helper_funs\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from nb_helper_funs import compile_n_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'Edi06'\n",
    "sample_folders = [Path(f'../data/raw/basaltic_glasses_StA/raw_quant_by_sample/{sample}/')]\n",
    "# List of folders corresponding to the samples\n",
    "category = 'basaltic glasses' # Category of this dataset (e.g. \"glasses\")\n",
    "wd_scan = Path(f'../data/interim/basaltic_glasses/fits/key_params_{sample}.txt') # Path to wd scan fit parameters\n",
    "std_dbase_info_file = Path('data/_dictionaries/standards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = readfiles.find_files_and_folders(\n",
    "                [sample], sample_folders,\n",
    "                # apf_file = None,\n",
    "                apf_file=Path('../data/_dictionaries/apf_values.csv'), #<- Can put None in here\n",
    "                wd_scan=wd_scan\n",
    "                )\n",
    "\n",
    "datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot = [None] * len(datalist.folder)\n",
    "\n",
    "for i in range(len(datalist.folder)):\n",
    "    peak, bg, standard, info = readfiles.read_and_organise_data(\n",
    "                                    datalist.loc[i,:].copy(),\n",
    "                                    bgi=False,\n",
    "                                    save=False)\n",
    "    myspot[i] = correct_quant.Spot()\n",
    "    myspot[i].add_data(info, bg, peak, standard)\n",
    "    myspot[i].add_wd_scan_params_from_file(wd_scan)\n",
    "    print('Read dataset:', i + 1, 'of', len(datalist), ':',\n",
    "          myspot[i].info.comment)\n",
    "    myspot[i].comprehensify_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_quant.process_datasets(\n",
    "    myspot, \n",
    "    datalist, \n",
    "    num_mc_sims=100, \n",
    "    path_out=Path(f\"../data/processed/basaltic_glasses/background_corrections/{sample}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write calczaf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [sample]\n",
    "category = 'basaltic glasses'\n",
    "subfolder = Path(f'../data/processed/basaltic_glasses/calczaf_files/{sample}/')\n",
    "\n",
    "write_detection_limit_calczaf_files = True\n",
    "detlim_subfolder = subfolder / Path('detlim')\n",
    "\n",
    "# note: in the subfolder there must be a file specifying valence.\n",
    "# this can be copied from the _dictionaries folder.\n",
    "valence_dict = readfiles.read_valence_file(subfolder, pattern='valence*')\n",
    "standard_database_dict = pd.read_csv(\n",
    "    '../data/_dictionaries/standards.csv',\n",
    "     index_col=0, \n",
    "     header=None, \n",
    "     squeeze=True).to_dict()\n",
    "\n",
    "standard_database_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary\n",
    "sampledata = {sample: myspot}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the major element analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majors_summary = pd.read_csv(\n",
    "    \"../data/processed/basaltic_glasses/basaltic_glasses_majors_summary.csv\",\n",
    "    header = [0, 1], index_col=0)\n",
    "majors_summary[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Oxygen and Total from the dataframe to get all other element values\n",
    "majors_relevant = majors_summary[sample].loc[\n",
    "    majors_summary[sample].index[~majors_summary[sample].index.isin([\"O\", \"Total\"])],\n",
    "    \"wt% mean\",\n",
    "]\n",
    "majors_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple different methods of processing the data, add a description\n",
    "run_descriptor = ['_1_base', '_2_bg', '_3_bg_apf']  \n",
    "# Leave as a list of an empty string if not using: e.g. run_descriptor = ['']\n",
    "\n",
    "for i in range(len(samples)):\n",
    "\n",
    "    # Here we pass in these arguments as a dictionary - this is useful in order\n",
    "    # to reuse the arguments for the detection limit function. But you can\n",
    "    # alternatively pass in each argument just by defining it in the function\n",
    "    # as normal (see glasses example).\n",
    "\n",
    "    args = {\n",
    "              'elementByDifference' : None # string element symbol\n",
    "            , 'elementByStoichToStoichOxygen' : None # string element symbol\n",
    "            , 'stoichOxygenRatio' : 0\n",
    "            # for hyalophane there is H\n",
    "            # that can be defined stoichiometrically relative to N:\n",
    "            , 'elementByStoichToOtherElement' : None\n",
    "            , 'OtherElement' : None\n",
    "            , 'stoichElementRatio' : None\n",
    "\n",
    "            , 'correct_bg' : False\n",
    "            , 'correct_apf' : False\n",
    "\n",
    "            # Elements to omit from matrix correction\n",
    "            # (e.g. if analysed but not actually present in sample)\n",
    "            , 'remove_elements' : None\n",
    "\n",
    "            , 'definedElements' : majors_relevant.index # list of element symbols to add\n",
    "            , 'definedElementWts' : majors_relevant.values # list of known element wt% to add\n",
    "            }\n",
    "    \n",
    "    # Make copies of args with different values\n",
    "    args2 = args.copy()\n",
    "    args2[\"correct_bg\"] = True\n",
    "    args2[\"correct_apf\"] = False\n",
    "\n",
    "    args3 = args2.copy()\n",
    "    args3[\"correct_bg\"] = True\n",
    "    args3[\"correct_apf\"] = True\n",
    "\n",
    "    args_list = [args, args2, args3]\n",
    "\n",
    "    for j in range(len(run_descriptor)):\n",
    "        print(\"******************************************************\")\n",
    "        print(args_list[j][\"correct_bg\"], args_list[j][\"correct_apf\"])\n",
    "        print(\"******************************************************\")\n",
    "\n",
    "        calczaf_path_out = subfolder / '{}{}.dat'.format(\n",
    "                                            samples[i], run_descriptor[j])\n",
    "        open(calczaf_path_out, 'w').close()  # Erase contents of file\n",
    "\n",
    "        if write_detection_limit_calczaf_files:\n",
    "            \n",
    "            detlim_path_out = detlim_subfolder / '{}{}_detlim.dat'.format(\n",
    "                                            samples[i], run_descriptor[j])\n",
    "            open(detlim_path_out, 'w').close()  # Erase contents of file\n",
    "\n",
    "        for spot in sampledata[samples[i]]:\n",
    "\n",
    "            calczaf.write_calczaf_input(\n",
    "                spot, calczaf_path_out, valence_dict, standard_database_dict,\n",
    "                accV=10, calcMode=2, taAngle=40, Oxide_or_Element=1,\n",
    "                **args_list[j]) # <- **args unpacks the args dictionary defined earlier\n",
    "                # so that all those arguments are passed into the function\n",
    "                # without the need to type them all out.\n",
    "\n",
    "            if write_detection_limit_calczaf_files:\n",
    "                if args_list[j]['correct_bg']:\n",
    "\n",
    "                    detlim_spot = correct_quant.create_detection_limit_spot(spot)\n",
    "\n",
    "                    calczaf.write_calczaf_input(\n",
    "                        detlim_spot, detlim_path_out, valence_dict, \n",
    "                        standard_database_dict,\n",
    "                        accV=10, calcMode=2, taAngle=40, Oxide_or_Element=1,\n",
    "                        **args_list[j])\n",
    "                    \n",
    "                else:\n",
    "                    print('\\n\\nWarning: Not writing detection limit file.' \n",
    "                            'Calculating detection limit does not make sense'\n",
    "                            ' except on background-corrected data. Raw data files' \n",
    "                            ' contain an estimate of detection limit without bg'\n",
    "                            ' correction.\\n')\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process calczaf outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = Path(f'../data/processed/basaltic_glasses/calczaf_files/{sample}/')\n",
    "\n",
    "helper_funs.check_calczaf_folder_exists(folderpath)\n",
    "valence_file = sorted(folderpath.glob('valence*'))[0]\n",
    "\n",
    "results = calczaf.process_calczaf_outputs(folderpath, valence_file)\n",
    "\n",
    "# For detection limits\n",
    "\n",
    "results_detlim = calczaf.process_calczaf_outputs(folderpath / 'detlim/', valence_file, detlim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tables = correct_quant.write_summary_excel_tables(\n",
    "    myspot, \n",
    "    f\"../data/processed/basaltic_glasses/kraw_summaries_{sample}.xlsx\"\n",
    "    )\n",
    "\n",
    "typical_kratios = pd.DataFrame({\n",
    "    \"K-ratio\": [\n",
    "        summary_tables[0][\"original.kraw_pcnt\"].mean(),\n",
    "        summary_tables[0][\"montecarlo.kraw_pcnt\"].mean(),\n",
    "        summary_tables[0][\"montecarlo.kraw_apf_pcnt\"].mean()\n",
    "    ],\n",
    "    \"Stdev % (relative)\": [\n",
    "        max(\n",
    "            summary_tables[0][\"original.kraw_stdev_pcnt\"].mean(),\n",
    "            summary_tables[0][\"original.kraw_pcnt\"].std()\n",
    "            ),\n",
    "        max(\n",
    "            summary_tables[0][\"montecarlo.kraw_stdev_pcnt\"].mean(),\n",
    "            summary_tables[0][\"montecarlo.kraw_pcnt\"].std()\n",
    "            ),\n",
    "        max(\n",
    "            summary_tables[0][\"montecarlo.kraw_stdev_apf_pcnt\"].mean(),\n",
    "            summary_tables[0][\"montecarlo.kraw_apf_pcnt\"].std()\n",
    "            )\n",
    "    ]\n",
    "}, index = [\n",
    "    \"Original K-ratio (%)\", \n",
    "    \"Bg-corrected K-ratio (%)\", \n",
    "    \"Bg- and APF-corrected K-ratio (%)\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "typical_kratios.insert(\n",
    "    1, \n",
    "    column=\"Stdev (absolute)\", \n",
    "    value = typical_kratios[\"K-ratio\"] * typical_kratios[\"Stdev % (relative)\"] / 100\n",
    "    )\n",
    "\n",
    "typical_kratios[\"filename\"] = results[\"wtdata\"].keys()\n",
    "typical_kratios.reset_index(inplace=True)\n",
    "typical_kratios.set_index(\"filename\", inplace=True)\n",
    "        # .loc[nm, \"Stdev % (relative)\"]/100\n",
    "# )\n",
    "typical_kratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all spots grouped by their method with stdev and detlims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"3_bg_apf\"\n",
    "stdev_string = \"kraw_stdev_apf_pcnt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"wtdata\"][f\"{sample}_{suffix}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_by_method = pd.DataFrame(\n",
    "    {\"comment\": datalist[\"comment\"],\n",
    "     \"N wt\": results[\"wtdata\"][f\"{sample}_{suffix}\"].loc[\"N\", list(range(0, len(datalist.index)))],\n",
    "     \"N detlim\": results_detlim[\"wtdata\"][f\"{sample}_{suffix}_detlim\"].loc[\"N\", list(range(0, len(datalist.index)))], \n",
    "     \"N stdev pct\": summary_tables[0][f\"montecarlo.{stdev_string}\"],\n",
    "     \"N stdev abs\": abs(\n",
    "         results[\"wtdata\"][f\"{sample}_{suffix}\"]\n",
    "        .loc[\"N\", list(range(0, len(datalist.index)))] * \n",
    "        summary_tables[0][f\"montecarlo.{stdev_string}\"]/100\n",
    "        )\n",
    "     }\n",
    "     )\n",
    "\n",
    "N_by_method[\"method\"] = N_by_method[\"comment\"].str.replace(r'_[0-9]+$', '', regex=True)\n",
    "N_by_method.groupby(\"method\")[\"N wt\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_by_method.round(3).sort_values(\"method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_by_method_sorted = N_by_method.sort_values(\"method\").reset_index()\n",
    "\n",
    "plt.errorbar(\n",
    "    x=N_by_method_sorted.index, \n",
    "    y=N_by_method_sorted[\"N wt\"] * 10000, \n",
    "    yerr=N_by_method_sorted[\"N stdev abs\"] * 10000,\n",
    "    marker=\"o\",\n",
    "    linestyle=\"\",\n",
    "    label=\"EPMA analysis\"\n",
    "    )\n",
    "\n",
    "plt.scatter(\n",
    "    x=N_by_method_sorted.index,\n",
    "    y=N_by_method_sorted[\"N detlim\"]*10000,\n",
    "    color=\"r\",\n",
    "    label=\"EPMA detection limit\",\n",
    "    zorder=0\n",
    ")\n",
    "# plt.xticks(range(0,13))\n",
    "# plt.ylim(0, 2000)\n",
    "plt.ylabel(\"N ppm\")\n",
    "plt.xlabel(\"Spot number\")\n",
    "\n",
    "plt.axhline(y=0, color=\"k\", label=\"Nominal value\")\n",
    "plt.legend()\n",
    "plt.title(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_summary = {}\n",
    "for nm in results[\"wtdata\"].keys():\n",
    "    n_summary[nm] = results[\"wtdata\"][nm].loc[\"N\", [\"average\", \"stdev\"]]\n",
    "    n_summary[nm].rename({\"average\": \"N wt% average\"}, inplace=True)\n",
    "    n_summary[nm].rename({\"stdev\": \"stdev (multiple measurements)\"}, inplace=True)\n",
    "    n_summary[nm][\"typical stdev on individual measurement\"] = (\n",
    "        n_summary[nm][\"N wt% average\"] * \n",
    "        typical_kratios\n",
    "        .loc[nm, \"Stdev % (relative)\"]/100\n",
    "    )\n",
    "\n",
    "pd.concat(n_summary, axis=1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_detlim[\"wtdata\"][f\"{sample}_3_bg_apf_detlim\"].loc[\"N\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_pct_summary_table = pd.concat(\n",
    "    {k: v[[\"average\", \"stdev\"]] for k, v in results[\"wtdata\"].items()},\n",
    "    axis=1\n",
    "    ).round(3)\n",
    "\n",
    "# wt_pct_summary_table.to_csv(\"../data/processed/hyalophane_StA/wt_pct_summary_GaNcalib.csv\")\n",
    "\n",
    "wt_pct_summary_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_list = [\"1_base\", \"2_bg\", \"3_bg_apf\"]\n",
    "\n",
    "summary, details = compile_n_summary(\n",
    "    suffix_list, results, results_detlim, sampledata, datalist, summary_tables, samples\n",
    ")\n",
    "\n",
    "summary.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(\"../data/processed/basaltic_glasses/N_summary_Edi06.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with Edi06 what we really need to do is look at the cps at the peak position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.peak.loc[0, \"raw_cps\"] for s in myspot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the raw cps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myspot[0].peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([s.peak.loc[0, \"raw_cps\"] for s in myspot]))\n",
    "(np.std([s.peak.loc[0, \"raw_cps\"] for s in myspot]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the typical raw cps in those Edi06 analyses was 64.5(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n_epma_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
